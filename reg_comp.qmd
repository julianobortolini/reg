---
title: "Análise de Regressão"
subtitle: "computacional"
author:
  - name: Prof. Dr. Juliano Bortolini
    email: profjulianobortolini@gmail.com
    url: http://www.julianobortolini.com.br
    affiliations:
      - name: Universidade Federal de Mato Grosso
format:
  html:
    embed-resources: true
engine: knitr
---

*Bacharelado em Estatística - UFMT*


# Modelo de Regressão Linear Simples

## Experimento

O artigo **“Ecofriendly Dyeing of Silk with Extract of Yerba Mate”** (Textile Res. J. 2017: 829–837) descreve um experimento para estudar os efeitos da concentração de corante (mg/L), da temperatura (°C) e do pH na adsorção de corante (mg de corante por grama de tecido). Adsorção de corante é um indicador de cor.


$$
Y = \beta_0 + \beta_1 X + \varepsilon
$$

- $Y$: adsorção  
- $X$: concentração
- $\varepsilon \sim N(0, \sigma^2)$

<!-- ::: {.cell-output-display} -->
<!-- <style> -->
<!-- .reveal pre code { -->
<!--   max-height: none !important; -->
<!--   overflow-y: visible !important; -->
<!--   white-space: pre-wrap; -->
<!--   font-size: 0.9em; -->
<!-- } -->
<!-- </style> -->
<!-- ::: -->

<!-- --- -->


```{r}
#| include: false
library(ggplot2)
library(viridis)   # paleta daltônica
library(ggrepel)   # rótulos inteligentes
library(patchwork) # combinar gráficos


theme_set(
  theme_minimal(base_size = 14) +
  theme(
    panel.grid = element_blank(),                     # remove todas as grades
    panel.border = element_blank(), # borda externa
    axis.line = element_line(color = "black")
    )
)
```



```{r}
dados <- data.frame(
  concentracao = c(10, 20, 20, 20, 10, 20, 10, 20, 15, 15),
  adsorcao = c(250, 520, 387, 593, 157, 377, 225, 451, 382, 373)
)
dados
```

<!-- --- -->

## Estatísticas descritivas

```{r}
summary(dados)
sd(dados$concentracao)
sd(dados$adsorcao)
```

<!-- --- -->

## Correlação entre as variáveis

```{r}
plot(dados$concentracao, dados$adsorcao)

ggplot(dados, aes(x = concentracao, y = adsorcao)) +
  geom_point() +
  labs(
    title = "Relação entre concentração e adsorção",
    x = "Concentração (mg/L)",
    y = "Adsorção (mg/g)"
  )

```


```{r}
cor(dados$concentracao, dados$adsorcao)
```

<!-- --- -->

## Teste de hipóteses:

$H_0: rho = 0$

```{r}
cor.test(dados$concentracao, dados$adsorcao)
```

- Observe o IC para $\rho$.

<!-- --- -->

## Ajuste do modelo

```{r}
modelo <- lm(adsorcao ~ concentracao, data = dados)
modelo
```

```{r}
anova(modelo)
```


<!-- --- -->

Estimativas, erros-padrões, estatísticas $t$, valores $p$,  $\sigma$, $R^2$,  $R^2_{aj}$,  $F_c$ para regressão (e valor $p$):

```{r}
summary(modelo)
```


<!-- --- -->

## Coeficientes (betas estimados)

```{r}
coef(modelo) # coeficientes
```


```{r}
confint(modelo) # IC para os coef.
```

- $\hat{\beta}_0$: adsorção média quando a concentração é zero
- $\hat{\beta}_1$: variação média da adsorção para cada unidade de aumento na concentração

<!-- --- -->

## Gráfico com a reta de regressão

```{r}

dados$ajustado <- fitted(modelo)

ggplot(dados, aes(x = concentracao, y = adsorcao)) +
  geom_point() +
  geom_line(aes(y = ajustado), color = "blue", linewidth = 1) +
  labs(
    title = "Relação entre concentração e adsorção",
    x = "Concentração (mg/L)",
    y = "Adsorção (mg/g)"
  )

plot(dados$concentracao, dados$adsorcao,
     main = "Concentração x Adsorção",
     xlab = "Concentração (mg/L)",
     ylab = "Adsorção (mg/g)")
abline(modelo, col = "blue", lwd = 2)
```

<!-- --- -->



## SQ e QM dos resíduos:

```{r}
residuos <- residuals(modelo)
SQRes <- sum(residuos^2)
SQRes
```

```{r}
GL_Res <- length(dados$adsorcao) - length(coef(modelo))
GL_Res
```

```{r}
QMRes <- SQRes / GL_Res
QMRes
sqrt(QMRes)
```

<!-- --- -->

## IC para resposta média

```{r}
preditos <- predict(modelo, interval = "confidence")
head(preditos)
colnames(preditos) <- c("adsorcao_estimada", "lim_inf", "lim_sup") 
head(preditos)
```

<!-- --- -->

```{r}
dados <- cbind(dados, preditos)
dados
```

<!-- --- -->

**Ordernar o banco de dados pela concentração:**

```{r}
dados <- dados[order(dados$concentracao),]
dados
```


<!-- --- -->

## Gráfico com IC para a resposta média

```{r}

grid <- data.frame(concentracao = seq(min(dados$concentracao),
                                      max(dados$concentracao),
                                      length.out = 200))
pred  <- as.data.frame(predict(modelo, newdata = grid, interval = "confidence"))
pred  <- cbind(grid, pred)


ggplot() +
  geom_ribbon(data = pred,
              aes(x = concentracao, ymin = lwr, ymax = upr),
              alpha = 0.18) +
  geom_line(data = pred,
            aes(x = concentracao, y = fit),
            color = "blue",
            linewidth = 1) +
  geom_point(data = dados,
             aes(x = concentracao, y = adsorcao),
             size = 3, alpha = 0.8) +
  labs(
    title = "Concentração x Adsorção",
    x = "Concentração (mg/L)",
    y = "Adsorção (mg/g)"
  )


plot(dados$concentracao, dados$adsorcao,
     main = "Concentração x Adsorção",
     xlab = "Concentração (mg/L)",
     ylab = "Adsorção (mg/g)",
     xlim = c(10,20),
     ylim = c(0, 600))

# valores estimados
lines(dados$concentracao, dados$adsorcao_estimada,
      col = "blue", lty = 1, lwd = 2) 
# IC
lines(dados$concentracao, dados$lim_inf,
      col = "blue", lty = 2, lwd = 2)
lines(dados$concentracao, dados$lim_sup,
      col = "blue", lty = 2, lwd = 2) 
```

<!-- --- -->

## IC para novas observações (IP)

```{r}

pred_new  <- as.data.frame(predict(modelo, newdata = grid, interval = "prediction"))
colnames(pred_new) <- c("fit", "lwr_new", "upr_new")
pred  <- cbind(pred, pred_new[,2:3])


ggplot() +
  geom_ribbon(data = pred,
              aes(x = concentracao, ymin = lwr, ymax = upr),
              alpha = 0.18) +
  geom_ribbon(data = pred,
              aes(x = concentracao, ymin = lwr_new, ymax = upr_new),
              alpha = 0.1) +
  geom_line(data = pred,
            aes(x = concentracao, y = fit),
            color = "blue",
            linewidth = 1) +
  geom_point(data = dados,
             aes(x = concentracao, y = adsorcao),
             size = 3, alpha = 0.8) +
  labs(
    title = "Concentração x Adsorção",
    x = "Concentração (mg/L)",
    y = "Adsorção (mg/g)"
  )



new_data <- data.frame(concentracao = seq(0,30, by = 1))
preditos_new <- predict(modelo, newdata = new_data,
                        interval = "prediction")
preditos_new <- cbind(new_data, preditos_new)
head(preditos_new)
```

<!-- --- -->

## Gráfico com IP para novas observações

```{r}
# dispersão
plot(dados$concentracao, dados$adsorcao,
     main = "Concentração x Adsorção",
     xlab = "Concentração (mg/L)",
     ylab = "Adsorção (mg/g)",
     xlim = c(0,30),
     ylim = c(-300, 1000))
# valores estimados
lines(preditos_new$concentracao, preditos_new$fit,
      col = "blue", lty = 1, lwd = 2) 
# IP
lines(preditos_new$concentracao, preditos_new$lwr,
      col = "brown", lty = 2, lwd = 2)
lines(preditos_new$concentracao, preditos_new$upr,
      col = "brown", lty = 2, lwd = 2) 

```


<!-- --- -->

## Gráfico com IC e IP

```{r}
# dispersão
plot(dados$concentracao, dados$adsorcao,
     main = "Concentração x Adsorção",
     xlab = "Concentração (mg/L)",
     ylab = "Adsorção (mg/g)",
     xlim = c(0,30),
     ylim = c(-300, 1000))

# valores estimados
lines(preditos_new$concentracao, preditos_new$fit,
      col = "blue", lty = 1, lwd = 2) 

# IC
lines(dados$concentracao, dados$lim_inf,
      col = "blue", lty = 2, lwd = 2)
lines(dados$concentracao, dados$lim_sup,
      col = "blue", lty = 2, lwd = 2)  

# IP
lines(preditos_new$concentracao, preditos_new$lwr,
      col = "brown", lty = 2, lwd = 2)
lines(preditos_new$concentracao, preditos_new$upr,
      col = "brown", lty = 2, lwd = 2) 
```


<!-- --- -->

<!-- ```{r, echo=FALSE} -->
<!-- # dispersão -->
<!-- plot(preditos_new$concentracao, preditos_new$fit, -->
<!--      main = "Concentração x Adsorção", -->
<!--      xlab = "Concentração (mg/L)", -->
<!--      ylab = "Adsorção (mg/g)", -->
<!--      xlim = c(0,30), -->
<!--      ylim = c(-300, 1000)) -->

<!-- # valores estimados -->
<!-- lines(preditos_new$concentracao, preditos_new$fit, -->
<!--       col = "blue", lty = 1, lwd = 2)  -->

<!-- # IC -->
<!-- lines(dados$concentracao, dados$lim_inf, -->
<!--       col = "blue", lty = 2, lwd = 2) -->
<!-- lines(dados$concentracao, dados$lim_sup, -->
<!--       col = "blue", lty = 2, lwd = 2)   -->

<!-- # IP -->
<!-- lines(preditos_new$concentracao, preditos_new$lwr, -->
<!--       col = "brown", lty = 2, lwd = 2) -->
<!-- lines(preditos_new$concentracao, preditos_new$upr, -->
<!--       col = "brown", lty = 2, lwd = 2)  -->
<!-- ``` -->

<!-- --- -->

## Regressão pela origem

```{r}
modelo2 <- lm(adsorcao ~ -1 + concentracao, data = dados)
modelo2
anova(modelo2)
```

<!-- --- -->

```{r}
summary(modelo2)
```

<!-- --- -->

## Comparação entre modelo completo e modelo reduzido (sem intercepto)

```{r}
anova(modelo2, modelo)
```

Se não há diferença entre os modelos, qual deve ser escolhido?

<!-- --- -->

## Comparação entre modelo completo e modelo reduzido (sem intercepto)

```{r}
anova(modelo2, modelo)
```

Se não há diferença entre os modelos, qual deve ser escolhido?


**O modelo mais simples: com menos parâmetros.**

<!-- --- -->

## Comparação de R2 e R2ajustado 


<!-- ::: columns -->

<!-- ::: column -->
```{r}
summary(modelo)
```
<!-- ::: -->

<!-- ::: column -->
```{r}
summary(modelo2)
```
<!-- ::: -->

<!-- ::: -->


<!-- --- -->

# Modelo de Regressão Linear Múltipla

$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 + \varepsilon
$$

- $Y$: adsorção  
- $X_1$: concentração
- $X_2$: temperatura
- $X_3$: pH
- $\varepsilon \sim N(0, \sigma^2)$


```{r}
dados <- data.frame(
  conc = c(10, 20, 20, 20, 10, 20, 10, 20, 15, 15),
  temp = c(70, 70, 80, 90, 70, 70, 90, 90, 80, 80),
  ph = c(3.0, 3.0, 3.0, 3.0, 4.0, 4.0, 4.0, 4.0, 3.5, 3.5),
  adsorcao = c(250, 520, 387, 593, 157, 377, 225, 451, 382, 373)
)
dados
```

<!-- --- -->

## Ajuste do modelo (regressão múltipla)

```{r}
# regressão múltipla
modelo3 <- lm(adsorcao ~ conc + temp + ph, data = dados)
modelo3
anova(modelo3)
```

<!-- --- -->

```{r}
coef(modelo3) # coeficientes
```


```{r}
confint(modelo3) # IC para os coef.

```


<!-- --- -->

Estimativas, erros-padrões, estatísticas $t$, valores $p$,  $\sigma$, $R^2$,  $R^2_{aj}$,  $F_c$ para regressão (e valor $p$):

```{r}
summary(modelo3)
```


<!-- --- -->

## IC para resposta média

```{r}
preditos_mod3 <- predict(modelo3, interval = "confidence")
head(preditos_mod3)
colnames(preditos_mod3) <- c("adsorcao_estimada", "lim_inf", "lim_sup") 
head(preditos_mod3)
```

<!-- --- -->

```{r}
dados <- cbind(dados, preditos_mod3)
dados
```

<!-- --- -->

## IC para novas observações (IP)

```{r}
new_data <- data.frame(conc = c(12, 12 , 25, 25),
                       temp = c(50, 80, 50, 80),
                       ph = c(6.0, 6.0, 6.0, 6.0))
preditos_new_mod3 <- predict(modelo3, newdata = new_data,
                        interval = "prediction")
preditos_new_mod3 <- cbind(new_data, preditos_new_mod3)
preditos_new_mod3
```


# Análise de resíduos



Example 3.1 The Delivery Time Data (Montgomery, Peck e Vining, 2021, p. 76)

> A soft drink bottler is analyzing the vending machine service routes in his distribution system. He is interested in predicting the amount of time required by the route driver to service the vending machines in an outlet. This service activity includes stocking the machine with beverage products and minor maintenance or housekeeping. The industrial engineer responsible for the study has suggested that the two most important variables affecting the delivery time (y) are the number of cases of product stocked (x1) and the distance walked by the route driver (x2). The engineer has collected 25 observations on delivery time:


```{r}
ID <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 
        11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 
        21, 22, 23, 24, 25)

y <- c(16.68, 11.50, 12.03, 14.88, 13.75, 18.11, 8.00, 17.83, 79.24, 21.50,
       40.33, 21.00, 13.50, 19.75, 24.00, 29.00, 15.35, 19.00, 9.50, 35.10,
       17.90, 52.32, 18.75, 19.83, 10.75)

x1 <- c(7, 3, 3, 4, 6, 7, 2, 7, 30, 5,
        16, 10, 4, 6, 9, 10, 6, 7, 3, 17,
        10, 26, 9, 8, 4)

x2 <- c(560, 220, 340, 80, 150, 330, 110, 210, 1460, 605,
        688, 215, 255, 462, 448, 776, 200, 132, 36, 770,
        140, 810, 450, 635, 150)

dados <- data.frame(ID, y, x1, x2)

```


```{r}
# Ajuste do modelo linear múltiplo
mod <- lm(y ~ x1 + x2, data = dados)

# estimativas dos parâmetros/coeficientes
summary(mod)

# Resíduos

# resíduos brutos
res_1 <- residuals(mod)

# resíduos padronizados
res_2 <- res_1/summary(mod)$sigma

# resíduos studentizados
# res_3 <- rstandard(mod, type = "sd.1")
res_3 <- rstandard(mod)

# resíduos PRESS
res_4 <- rstandard(mod, type = "pred")

# resíduos R-Student
res_5 <- rstudent(mod)

residuos <- data.frame(
  "brutos" = res_1,
  "padronizados" = res_2,
  "studentizado" = res_3,
  "PRESS" = res_4,
  "RStudent" = res_5
)

residuos
```



Nas análises de resíduos é indicado usar o resíduo studentizado ou o resíduo R-Student. É esperado que os valores dos resíduos sejam uma amostra da distribuição $t$ com $n-p-1$ graus de liberdade. Para tamanho amostral suficientemente grande, pode-se comparar com a distribuição normal padrão. 


## Gráficos de resíduos

```{r}
# Gráfico qqnorm
qqnorm(res_3)
qqline(res_3, col = "blue", lwd = 2)


# Gráfico de dispersão y_est x resíduos
y_est <- fitted(mod)
plot(y_est, res_3, xlim = c(0,80), ylim = c(-2,5),
     main = "Gráfico de dispersão y_est x resíduos",
     xlab = "y estimado",
     ylab = "Resíduos studentizados")

# Gráfico dos resíduos seguindo a ordem de coleta
# ou execução dos dados
# atenção: supondo que a coleta tenha sido na ordem da variável ID, o que nem sempre é verdade!
plot(res_3 ~ dados$ID)

```


## Testes para os resíduos

### Normalidade dos resíduos


Alguns testes são: Qui-quadrado, **Kolmogorov-Smirnov**, Jarque-Bera, **Shapiro-Wilk**, Anderson-Darling, Cramér-von Mises, D'Agostino-Pearson, Lilliefors, e Shapiro-Francia.

```{r}
# Teste de Shapiro-Wilk
shapiro.test(res_3)

# Teste de Kolmogorov-Smirnov
ks.test(res_3, "pnorm", mean = 0, sd = 1)
```


### Independência dos Resíduos


Alguns testes são: **Durbin-Watson**, Breusch-Godfrey e Ljung-Box.

Para testar a independência dos resíduos é necessário conhecer a ordem de coleta (ou execução) dos dados.

Supondo que a coleta dos dados tenha sido conforme a variável $ID$ (no exemplo delivery time), então, o teste DW é:

```{r message=FALSE}
library(lmtest)
dwtest(res_3 ~ dados$ID, alternative = "two.sided")
```



### Homocedasticidade dos Resíduos

Alguns testes são: Bartlett, **Breusch-Pagan**, Levene, Samiuddin, O'Neill e Mathews, Layard, Park, White, Cochran, Hartley e Goldfeld-Quandt.


```{r}
# Teste de Breusch-Pagan
# library(lmtest)
bptest(mod)
```



## Valores discrepantes ou outliers
```{r}
# Análise de outliers
cbind(dados, res_3)
# é necessário verificar se a observação 9 é um outlier.


# gráfico dos resíduos
plot(res_3, main = "Resíduos studentizados",
     ylab = "Resíduos studentizados",
     xlab = "Observações")
abline(h = 2, col = "red", lty = 2)
abline(h = -2, col = "red", lty = 2)


# Se for, de fato, um outlier, o que fazer?
# Estimar novamente o modelo sem a observação 9.

mod_2 <- lm(y ~ x1 + x2, data = dados[-9,])
summary(mod_2)


# comparar os resultados dos dois modelos:
summary(mod)
summary(mod_2)


# Estatística PRESS
PRESS_mod <- sum(res_4^2)
PRESS_mod


res_4_mod_2 <- rstandard(mod_2, type = "pred")
PRESS_mod_2 <- sum(res_4_mod_2^2)
PRESS_mod_2

# Comparar as estatísticas PRESS
PRESS_mod
PRESS_mod_2
```

## Pontos de alavancagem (leverage points)

```{r}
# valores matriz H
p <- 3 # qde de parâmetros (b0, b1 e b2)
#ou
p <- length(coef(mod))


n <- 25 # qde de observaçoes
# ou
n <- length(dados$y)

2*p/n  # limite para valores da matriz hat
h <- hatvalues(mod)
cbind(dados, h, 2*p/n, h > 2*p/n)

# observações 9 e 22 são possíveis pontos de alavancagem.
```


## Distância de Cook

```{r}
# Distância de Cook
D <- cooks.distance(mod)

# comparar com o valor 1
# Se D > 1, a observação é considerada um ponto influente.

cbind(dados, D, 1, D > 1)
```

## DFBETAS

```{r}
# DFBETAS
# a influência de cada observação em cada coeficiente
# comparar com o valor 2/sqrt(n)

dfbetas(mod)
2/sqrt(n)
round(dfbetas(mod), 2)

abs(dfbetas(mod)) >= 2/sqrt(n)
```


## DFFITS

```{r}
# DFFITS
# a influência de cada observação em cada predição
# comparar com o valor 2*sqrt(p/n)

dffits(mod)
2*sqrt(p/n)
round(dffits(mod), 2)

abs(dffits(mod)) >= 2*sqrt(p/n)
```



### Exercício

Considere os dados da Liga Nacional de Futebol Americano (NFL) na Tabela 1.

```{r}
equipe <- c("Washington", "Minnesota", "New England", "Oakland", "Pittsburgh", 
            "Baltimore", "Los Angeles", "Dallas", "Atlanta", "Buffalo",
            "Chicago", "Cincinnati", "Cleveland", "Denver", "Detroit",
            "Green Bay", "Houston", "Kansas City", "Miami", "New Orleans",
            "New York Giants", "New York Jets", "Philadelphia", "St. Louis", "San Diego",
            "San Francisco", "Seattle", "Tampa Bay")

y <- c(10, 11, 11, 13, 10,
       11, 10, 11, 4, 2,
       7, 10, 9, 9, 6,
       5, 5, 5, 6, 4, 
       3, 3, 4, 10, 6, 
       8, 2, 0)

x1 <- c(2113, 2003, 2957, 2285, 2971,
        2309, 2528, 2147, 1689, 2566,
        2363, 2109, 2295, 1932, 2128,
        1722, 1498, 1873, 2118, 1775,
        1904, 1929, 2080, 2301, 2040,
        2447, 1416, 1503)

x2 <- c(1985, 2855, 1737, 2905, 1666,
        2927, 2341, 2737, 1414, 1838,
        1480, 2191, 2229, 2204, 2438,
        1730, 2072, 2929, 2268, 1983,
        1792, 1606, 1492, 2835, 2416,
        1638, 2649, 1503)

x3 <- c(38.9, 38.8, 40.1, 41.6, 39.2,
        39.7, 38.1, 37.0, 42.1, 42.3,
        37.3, 39.5, 37.4, 35.1, 38.8,
        36.6, 35.3, 41.1, 38.2, 39.3,
        39.7, 39.7, 35.5, 35.3, 38.7,
        39.9, 37.4, 39.3)


x4 <- c(64.7, 61.3, 60.0, 45.3, 53.8,
        74.1, 65.4, 78.3, 47.6, 54.2,
        48.0, 51.9, 53.6, 71.4, 58.3,
        52.6, 59.3, 55.3, 69.6, 78.3,
        38.1, 68.8, 68.8, 74.1, 50.0,
        57.1, 56.3, 47.0)

x5 <- c(4, 3, 14, -4, 15,
        8, 12, -1, -3, -1,
        19, 6, -5, 3, 6,
        -19, -5, 10, 6, 7,
        -9, -21, -8, 2, 0,
        -8, -22, -9)

x6 <- c(868, 615, 914, 957, 836,
        786, 754, 761, 714, 797,
        984, 700, 1037, 986, 819,
        791, 776, 789, 582, 901,
        734, 627, 722, 683, 576,
        848, 684, 875)

x7 <- c(59.7, 55.0, 65.6, 61.4, 66.1,
        61.0, 66.1, 58.0, 57.0, 58.9,
        67.5, 57.2, 57.8, 58.6, 59.2,
        54.4, 49.6, 54.3, 58.7, 51.7,
        61.9, 52.7, 57.8, 59.7, 54.9,
        65.3, 43.8, 53.5)

x8 <- c(2205, 2096, 1847, 1903, 1457,
        1848, 1564, 1821, 2577, 2476,
        1984, 1917, 1761, 1790, 1901,
        2288, 2072, 2861, 2411, 2289,
        2203, 2592, 2053, 1979, 2048,
        1786, 2876, 2560)

x9 <- c(1917, 1575, 2175, 2476, 1866,
        2339, 2092, 1909, 2001, 2254,
        2217, 1758, 2032, 2025, 1686,
        1835, 1914, 2496, 2670, 2202,
        1988, 2324, 2550, 2110, 2628,
        1776, 2524, 2241)




tabela1 <- data.frame(equipe, y, x1, x2, x3, x4, x5, x6, x7, x8, x9)

knitr::kable(tabela1, caption = "Tabela 1: Desempenho das Equipes da National Football League de 1976")
```


Definições das variáveis:

-   $y$: Jogos vencidos (por temporada de 14 jogos)
-   $x_1$: Jardas conquistadas em corrida (temporada)
-   $x_2$: Jardas de passe (temporada)
-   $x_3$: Média de punt (jardas/punt)
-   $x_4$: Percentual de acerto em field goal (field goals convertidos/field goals tentados na temporada)
-   $x_5$: Diferença de turnovers (turnovers ganhos - turnovers perdidos)
-   $x_6$: Jardas de penalidade (temporada)
-   $x_7$: Percentual de corrida (corridas/jogadas totais)
-   $x_8$: Jardas conquistadas em corrida pelos adversários (temporada)
-   $x_9$: Jardas de passe dos adversários (temporada)

a.  Ajuste um modelo de regressão linear múltipla relacionando o número de jogos ganhos com as jardas aéreas do time ($x_2$), a porcentagem de jogadas terrestres ($x_7$) e as jardas terrestres dos adversários ($x_8$).

```{r}
m1 <- lm(y  ~ x2 + x7 + x8, data = tabela1)
m1
```


b.  Construa a tabela de análise de variância (ANOVA) e teste a significância da regressão.

```{r}
anova(m1)
```


c.  Calcule as estatísticas $t$ para testar as hipóteses: $H_0: \beta_2 = 0$, $H_0: \beta_7 = 0$ e $H_0: \beta_8 = 0$. Quais conclusões você pode tirar sobre os papéis das variáveis $x_2$, $x_7$ e $x_8$ no modelo?


```{r}
summary(m1)
```


d.  Calcule o $R^2$ e o $R^2_{\text{aj}}$ (ajustado) para este modelo.

```{r}
summary(m1)
names(summary(m1))
summary(m1)$r.squared
summary(m1)$adj.r.squared
```


e.  Usando o teste $F$ parcial, determine a contribuição de $x_7$ para o modelo. Como essa estatística $F$ parcial está relacionada ao teste $t$ para $\beta_7$ calculado no item c?

```{r}
m2 <- lm(y ~ x2 + x8, data = tabela1)
m2
anova(m2, m1)
```


f.  Construa um gráfico de probabilidade normal dos resíduos. Há indícios de violação da suposição de normalidade? Realize os testes de normalidade de Shapiro-Wilk e Kolmogorov-Smirnov. O que você conclui?

```{r}
res_m1 <- rstandard(m1) # resíduos studentizados
qqnorm(res_m1)
qqline(res_m1)
shapiro.test(res_m1)
ks.test(res_m1, "pnorm", mean = 0, sd = 1)
```



g.  Calcule os resíduos studentizados e os resíduos Rstudent para este modelo. Que tipo de informação é fornecida por esses resíduos?

```{r}
res_m1 # resíduos studentizados já calculados
# resíduos R-Student
res_m1_rstudent <- rstudent(m1)
cbind(res_m1, res_m1_rstudent)
```



h.  Construa e interprete o gráfico dos resíduos studentizados em função da resposta predita. Realize o teste de homocedasticidade de Breusch-Pagan. O que você conclui?


```{r}
y_estimado <- fitted(m1) # valores ajustados
plot(y_estimado, res_m1,
     xlab = "Valores ajustados",
     ylab = "Resíduos studentizados"
     )
abline(h = -2, col = "red", lty = 2)
abline(h = 2, col = "red", lty = 2)
library(lmtest)
bptest(m1)
```


i.  Calcule as correlações entre as variáveis independentes $x_2$, $x_7$ e $x_8$. Existe multicolinearidade entre essas variáveis? Justifique sua resposta.

```{r}
cor(tabela1[, c("x2", "x7", "x8")], use = "pairwise.complete.obs")
```



j.  Calcule o VIF (Variance Inflation Factor) para as variáveis $x_2$, $x_7$ e $x_8$. O que você conclui? Os valores do VIF são úteis para indicar multicolinearidade. Valores de VIF superior a 5 ou 10 sugerem multicolinearidade. *Use a função `vif()` do pacote `car` para calcular o VIF.*


```{r}
library(car)
vif(m1)
```


k.  Calcule a estatística PRESS (Press Residual Sum of Squares) para os modelos completo (com $x_2$, $x_7$ e $x_8$) e reduzido (apenas com $x_2$ e $x_8$). O que você conclui?


```{r}
# Cálculo PRESS m1
res_press_m1 <- rstandard(m1, type = "pred")
PRESS_m1 <- sum(res_press_m1^2)
PRESS_m1

# Cálculo PRESS m2
res_press_m2 <- rstandard(m2, type = "pred")
PRESS_m2 <- sum(res_press_m2^2)
PRESS_m2
```


l.  Calcule os valores dos pontos de alavancagem (leverage). Quais observações são consideradas influentes? **Lembre-se que valores superiores a** $2p/n$ (onde $p$ é o número de parâmetros do modelo e $n$ é o número de observações) são considerados influentes.


```{r}
# valores matriz H
p <- length(coef(m1)) # qde de parâmetros (b0, b1, b2 e b3)


n <- nrow(tabela1) # qde de observaçoes

2*p/n  # limite para valores da matriz hat
h <- hatvalues(m1)
cbind(tabela1[, c("y", "x2", "x7", "x8")], y_estimado, h, 2*p/n, h > 2*p/n)

which(h > 2*p/n)
```



m.  Calcule os valores da distância de Cook. Quais observações são consideradas influentes? **Lembre-se que valores superiores a 1 são considerados influentes.**


```{r}
# Distância de Cook
D <- cooks.distance(m1)

# comparar com o valor 1
# Se D > 1, a observação é considerada um ponto influente.

cbind(tabela1[, c("y", "x2", "x7", "x8")], y_estimado, D, 1, D > 1)
```


n.  Calcule os valores de DFBETAS. Quais observações são consideradas influentes? **Lembre-se que valores superiores a** $2/\sqrt{n}$ merecem atenção.



```{r}
# DFBETAS
# a influência de cada observação em cada coeficiente
# comparar com o valor 2/sqrt(n)

dfbetas(m1)
2/sqrt(n)
round(dfbetas(m1), 2)

abs(dfbetas(m1)) >= 2/sqrt(n)
```


o.  Calcule os valores de DFFITS. Quais observações são consideradas influentes? **Lembre-se que valores superiores a** $2\sqrt{p/n}$ merecem atenção.



```{r}
# DFFITS
# a influência de cada observação em cada predição
# comparar com o valor 2*sqrt(p/n)

dffits(m1)
2*sqrt(p/n)
round(dffits(m1), 2)

abs(dffits(m1)) >= 2*sqrt(p/n)
```


p.  Construa um modelo de regressão linear relacionando o número de jogos vencidos com:

-   jardas terrestres dos adversários ($x_8$),

-   porcentagem de jogadas terrestres ($x_7$) e

-   *diferença de turnovers (turnovers ganhos - turnovers perdidos) (*$x_5$).

Especificamente, *considere a diferença de turnovers como uma variável indicadora*, cuja codificação depende de o valor real do diferencial ser positivo, negativo ou igual a zero. Ou seja, considere $x_5$ como uma variável indicadora que assume os seguintes valores:

\begin{align*}
  x_5 = 1, & \text{ se } x_5 > 0 \\
  x_5 = 0, & \text{ se } x_5 = 0 \\
  x_5 = -1, & \text{ se } x_5 < 0.
\end{align*}

Quais conclusões podem ser obtidas sobre o efeito dos turnovers no número de jogos vencidos?

```{r}
I_x5 <- ifelse(x5 > 0, 1, ifelse(x5 == 0, 0, -1))
I_x5 <- as.factor(I_x5)

m3 <- lm(y ~ x7 + x8 + I_x5)
summary(m3)
```


q.  Considerando o enunciado da letra anterior, e se a variável indicadora for definida como: 

\begin{align*}
  x_5 = 1, & \text{ se } x_5 > 0 \\
  x_5 = 0, & \text{ se } x_5 \leq 0.
\end{align*}

Qual é a interpretação do coeficiente estimado para $x_5$? O que você conclui sobre o efeito dos turnovers no número de jogos vencidos?

```{r}
I2_x5 <- ifelse(x5 > 0, 1, 0)
I2_x5 <- as.factor(I2_x5)
m4 <- lm(y ~ x7 + x8 + I2_x5)
summary(m4)
```

r.  Considerando todas as variáveis disponíveis, utilize o método de seleção de variáveis forward (seleção progressiva) para selecionar um modelo de regressão com subconjunto de variáveis. Comente sobre o modelo final. **O método de seleção de variáveis forward (seleção progressiva) é um método de seleção de variáveis que começa com um modelo nulo (sem variáveis preditoras) e adiciona variáveis preditoras uma a uma, com base em critérios estatísticos, até que não haja mais variáveis significativas a serem adicionadas.**


```{r}
# Modelo reduzido
# Modelo mínimo a ser considerado.
# O modelo mínimo pode ser um modelo nulo (sem variáveis preditoras)
modelo_reduzido <- lm(y ~ 1, data = tabela1)

# Forward selection
modelo_forward <- step(modelo_reduzido,
    scope = as.formula(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9),
    direction = "forward")
summary(modelo_forward)
```


s.  Utilize o método de seleção de variáveis backward (eliminacão progressiva) para selecionar um modelo de regressão com subconjunto de variáveis. Comente sobre o modelo final. **O método de seleção de variáveis backward (eliminacão progressiva) é um método de seleção de variáveis que começa com um modelo completo (com todas as variáveis preditoras) e remove variáveis preditoras uma a uma, com base em critérios estatísticos, até que não haja mais variáveis insignificativas a serem removidas.**

```{r}
# Modelo completo
# Modelo máximo a ser considerado.
# O modelo máximo pode ser um modelo completo (com todas as variáveis preditoras)

modelo_completo <- lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8 + x9,
                      data = tabela1)

# Backward elimination
modelo_backward <- step(modelo_completo,
                        direction = "backward")
summary(modelo_backward)
```



t.  Utilize o método de seleção de variáveis stepwise (passo a passo) para selecionar um modelo de regressão com subconjunto de variáveis. Comente sobre o modelo final. **O método de seleção de variáveis stepwise (passo a passo) é um método de seleção de variáveis que combina os métodos forward e backward. Ele começa com um modelo nulo (sem variáveis preditoras) e adiciona variáveis preditoras uma a uma, com base em critérios estatísticos, mas também pode remover variáveis preditoras que se tornam não significativas à medida que novas variáveis são adicionadas.**

```{r}


# Stepwise regression
modelo_stepwise <- step(modelo_reduzido,
        scope = formula(modelo_completo),
        direction = "both")

summary(modelo_stepwise)
```

u.  Usando o método de validação cruzada *leave-one-out* (LOOCV), estime o erro de previsão para os modelos (1) com as variáveis regressoras $x_2$, $x_7$ e $x_8$ e (2) com as variáveis regressoras $x_2$ e $x_8$. O que você conclui? **O método de validação cruzada leave-one-out (LOOCV) é uma técnica de validação cruzada onde cada observação do conjunto de dados é usada como um ponto de teste, enquanto todas as outras observações são usadas como pontos de treinamento. Isso é repetido para cada observação, e o erro médio de previsão é calculado. Existe também o método leave-p-out.**

```{r}
library(boot)
# LOOCV
m1 <- glm(y ~ x2 + x7 + x8,
              data = tabela1,
              family = gaussian(link = "identity"))
K <- nrow(tabela1) # k subconjuntos (folds)
cv_m1 <- cv.glm(data = tabela1, glmfit = m1, K = K)
cv_m1$delta  # delta[1] = erro de validação cruzada


m2 <- glm(y ~ x2 + x8,
              data = tabela1,
              family = gaussian(link = "identity"))
cv_m2 <- cv.glm(data = tabela1, glmfit = m2, K = K)
cv_m2$delta  # delta[1] = erro de validação cruzada
```

v.  Usando o método de validação cruzada *k-fold*, com $k = 3$, estime o erro de previsão para os modelos (1) com as variáveis regressoras $x_2$, $x_7$ e $x_8$ e (2) com as variáveis regressoras $x_2$ e $x_8$. O que você conclui? **O método de validação cruzada k-fold é uma técnica de validação cruzada onde o conjunto de dados é dividido em k subconjuntos (folds). O modelo é treinado em k-1 folds e testado no fold restante. Isso é repetido k vezes, e o erro médio de previsão é calculado.**

```{r}
# k-fold
m1 <- glm(y ~ x2 + x7 + x8,
              data = tabela1,
              family = gaussian(link = "identity"))

K <- 3 # k subconjuntos (folds)
cv_m1 <- cv.glm(data = tabela1, glmfit = m1, K = K)
cv_m1$delta  # delta[1] = erro de validação cruzada


m2 <- glm(y ~ x2 + x8,
              data = tabela1,
              family = gaussian(link = "identity"))
cv_m2 <- cv.glm(data = tabela1, glmfit = m2, K = K)
cv_m2$delta  # delta[1] = erro de validação cruzada
```



------------------------------------------------------------------------

::: {style="text-align: center;"}
[\@profjulianobortolini](https://instagram.com/profjulianobortolini)      [www.julianobortolini.com.br](http://www.julianobortolini.com.br)      [linkedin](https://linkedin.com/in/julianobortolini)      [github](https://github.com/julianobortolini)       [lattes](http://lattes.cnpq.br/6210909768845403)
:::

